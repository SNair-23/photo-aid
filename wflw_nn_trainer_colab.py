# -*- coding: utf-8 -*-
"""WFLW_NN_Trainer_Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_dXCJkMsFt6vrwoZXCks5hQXr6LpyDq
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers, models, callbacks
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import cv2
import ast

import sklearn
from sklearn.utils import class_weight

from keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

from google.colab import drive
drive.mount('/content/drive')

#CONSTANTS
tar_path = "/content/drive/MyDrive/WFLW_images.tar.gz"
extract_path = "/content/WFLW_images"
image_dir = "/content/WFLW_images/WFLW_images"

"""Extracts annotations from a .tar.gz file into a separate path"""
def extract_tar_gz(tar_path="/content/drive/MyDrive/WFLW_images.tar.gz", extract_path="/content/WFLW_images"):
  import tarfile
  import os

  # Ensure the extraction directory exists
  os.makedirs(extract_path, exist_ok=True)

  with tarfile.open(tar_path, "r:gz") as tar:
      # Extract all contents to the specified path
      tar.extractall(path=extract_path)

  print(f"Extracted contents to: {extract_path}")

def read_csv():
  csv_data = pd.read_csv('saving.csv')
  return csv_data


def plot_csv_distribution(csv_data):
  data = read_csv()
  categories = ('blur', 'occlusion', 'make_up', 'illumination', 'expression', 'pose')
  ones = []
  zeros = []
  for category in categories:
      s =  data[category].value_counts(normalize=True)
      ones.append(s[1])
      zeros.append(s[0])


  weight_counts = {
      "0 (Normal)": np.array(zeros),
      "1 (Extreme)": np.array(ones),
  }

  colors = {
    "0 (Normal)": "#75c5d2",
    "1 (Extreme)": "#f5abb8",
  }
  width = 0.5

  fig, ax = plt.subplots()
  bottom = np.zeros(6)

  for boolean, weight_count in weight_counts.items():
      p = ax.bar(categories, weight_count, width, label=boolean, bottom=bottom, color=colors[boolean])
      bottom += weight_count

  ax.set_title("Distribution of Binary Labels per Attribute")
  ax.legend(loc="upper right")

  plt.show()

"""
#Test cases:
csv_data = read_csv()
plot_csv_distribution(csv_data)
"""



from sklearn.utils.class_weight import compute_class_weight

def get_class_weights(labels):
    labels = np.array(labels)
    classes = np.unique(labels)
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)
    return dict(zip(classes, weights))


#print("Class weights:", get_class_weights([0, 0, 0, 0, 1, 0, 1])) # Example usage

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, DepthwiseConv2D, MaxPooling2D, Rescaling

model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255, input_shape=(96, 96, 3)),

    tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Conv2D(32, (1, 1), activation='relu', padding='same'), #changing this to increase dimension
    tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification

])

tf.keras.metrics.PrecisionAtRecall(
  0.5, num_thresholds=200, class_id=None, name=None, dtype=None)

def load_images(csv_data, image_dir="/content/WFLW_images/WFLW_images"):
  loaded_images = []
  for i in range(len(csv_data)):
    img_path = csv_data['file_name'][i] #creates a path using the folder(drive) and the filename as found in the csv
    img_path = image_dir + "/" + img_path
    if os.path.exists(img_path): # Check if the file exists
      img = cv2.imread(img_path)
      img = cv2.resize(img, (96, 96))
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      if img is not None:
        loaded_images.append(img)
      else:
        print(f"Image not found: {img_path}")
        loaded_images.append(None)

  return loaded_images

def make_binary_NN(csv_data, attribute, epoch, image_dir="/content/WFLW_images/WFLW_images", lr=0.0028):
    tf.keras.optimizers.Adam(learning_rate=lr)
    loaded_images = load_images(csv_data, image_dir)
    # Filter csv_data to include only rows for which images were successfully loaded
    # Assuming loaded_images were added in the same order as csv_data was iterated
    valid_indices = [i for i, img in enumerate(loaded_images) if img is not None]
    filtered_csv_data = csv_data.iloc[valid_indices].reset_index(drop=True)


    training_labels = filtered_csv_data[attribute].to_numpy()
    # Convert labels to a numerical format (e.g., integers)
    training_labels_numerical = training_labels.astype(int)


    # Convert the list of images to a NumPy array
    training_images_array = np.array(loaded_images)

    """ Stops model too early
    elst =  callbacks.EarlyStopping(
        monitor='val_loss',
        min_delta=0,
        patience=20,
        verbose=1,
        mode='min',
        baseline=None,
        restore_best_weights=True
    )
    """

    chkpt = callbacks.ModelCheckpoint(
        filepath= attribute + ".keras",
        monitor='val_loss',
        save_best_only=True,
        mode='min'
    )

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',

        metrics=['accuracy', 'precision', 'recall', tf.keras.metrics.PrecisionAtRecall(0.8)]
    )

    history=model.fit(
        training_images_array,
        training_labels_numerical,
        batch_size=50,
        epochs=epoch,
        class_weight=get_class_weights(training_labels_numerical),
        validation_split=0.2,
        callbacks=[chkpt]
    )
    return history

from typing_extensions import runtime
#runs and times the model on one attribute
def run_model(csv_data, attribute, epoch, image_dir = "/content/WFLW_images/WFLW_images", lr=0.00355):
  import time
  start_time = time.time()
  history = make_binary_NN(csv_data, attribute, epoch, image_dir, lr)
  end_time = time.time()
  print(f"Time taken: {end_time - start_time} seconds")
  return end_time - start_time, history.history['accuracy']

#Runs the training on a single attribute (run_model) then plots the accuracy over epochs
def run_and_plot(csv_data, attribute, epoch, image_dir = "/content/WFLW_images/WFLW_images", lr=0.0028):
  run_time, accuracy = run_model(csv_data, attribute, epoch, image_dir, lr)
  x_ticks = np.linspace(0, len(accuracy), len(accuracy))
  plt.plot(x_ticks, accuracy)
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.title('Accuracy per Epoch')
  plt.text(-10, 1.25, "Attribute: " + str(attribute) + "   Epochs: " + str(epoch) + "    Batch-size: 50    LR: " + str(lr), fontsize=11, color='black')
  plt.text(-10, 1.20, "With 1 Conv2D layer between 3 DepthwiseConv2D layers", fontsize=11, color='black')
  plt.text(-10, 1.15, "Runtime (GPU): " + str(run_time) + " seconds", fontsize=11, color='black')
  plt.text(-10, 1.10, "Maximum Accuracy: " + str(max(accuracy)), fontsize=11, color='black')

#Experiment --- speed of program in 5 runs within one runtime VS 100 epochs in one run
#Displays the improvement of accuracy after each run within the same runtime
def run_5_times(csv_data, epoch, attribute):
  import time
  run_time1, accuracy1 = run_model(csv_data, epoch, attribute)
  time.sleep(5)
  run_time2, accuracy2 = run_model(csv_data, epoch, attribute)
  time.sleep(5)
  run_time3, accuracy3 = run_model(csv_data, epoch, attribute)
  time.sleep(5)
  run_time4, accuracy4 = run_model(csv_data, epoch, attribute)
  time.sleep(5)
  run_time5, accuracy5 = run_model(csv_data, epoch, attribute, image_dir, lr=0.001)

  plt.figure(figsize=(6,6))
  plt.tight_layout()
  epochs = len(accuracy1)
  y1, y2, y3, y4, y5 = accuracy1, accuracy2, accuracy3, accuracy4, accuracy5
  x_ticks = np.linspace(0, epochs, epochs)
  plt.plot(x_ticks, y1, color='blue', label='run1')
  plt.plot(x_ticks, y2, color='red', label='run2')
  plt.plot(x_ticks, y3, color='green', label='run3')
  plt.plot(x_ticks, y4, color='orange', label='run4')
  plt.plot(x_ticks, y5, color='black', label='run5')
  plt.legend()
  total_run_time = run_time1 + run_time2 + run_time3 + run_time4
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.title('Accuracy per Epoch over 5 Runs')
  plt.text(-1.5, 1.25, "Attribute: " + str(attribute) + "   Epochs: " + str(epoch) + "    Batch-size: 100    LR: 0.0028", fontsize=11, color='black')
  plt.text(-1.5, 1.20, "The learning rate is 0.001 on run5", fontsize=11, color='black' )
  plt.text(-1.5, 1.15, "With 1 Conv2D layer between 3 DepthwiseConv2D layers", fontsize=11, color='black')
  plt.text(-1.5, 1.10, "Runtime (CPU): " + str(total_run_time) + " seconds", fontsize=11, color='black')
  plt.text(-1.5, 1.05, "Latest Accuracy: " + str(y5[-1]), fontsize=11, color='black')
  plt.show()

#Test -- ensure that order of attributes listed does not effect rate of loss...
#Runs the attributes in order of blur to pose, then pose to blur, and then plots the last accuracy of each
def run_all_fwd_rev(csv_data, epoch):

  print("----------------------FORWARD--------------------------- ")
  fwd_run_times = [] # Initialize an empty list to store runtimes
  fwd_accuracies = {}
  for attribute in csv_data.columns[2:8]:
    print("Now running ", attribute)
    x, y = run_model(csv_data, attribute, epoch, image_dir)
    fwd_run_times.append(x) # Append the runtime to the list
    fwd_accuracies[attribute] = y[-1] # Store the last accuracy from the history
  sum1 = sum(fwd_run_times)
  print("Total runtime for all attributes (GPU): ", sum1)

  import time
  time.sleep(60)

  print("----------------------REVERSE--------------------------- ")
  rev_run_times = [] # Initialize an empty list to store runtimes
  rev_accuracies = {}
  for attribute in csv_data.columns[2:8][::-1]: # Convert to list and reverse
    print("Now running ", attribute)
    w, z = run_model(csv_data, attribute, epoch, image_dir)
    rev_run_times.append(w) # Append the runtime to the list
    rev_accuracies[attribute] = z[-1] # Store the last accuracy from the history
  sum2 = sum(rev_run_times)
  print("Total runtime for attributes in reverse (GPU): ", sum2)

  plt.figure(figsize=(10,6))
  attributes = list(fwd_accuracies.keys())
  for i, attr in enumerate(attributes):
        plt.plot([i, i], [0, fwd_accuracies[attr]], color='blue', linewidth=2)
        #plt.plot([i, i], [0, rev_accuracies[attr]], color='red', linewidth=2)
        plt.scatter(i, fwd_accuracies[attr], color='blue', label='Forward' if i == 0 else "")
        #plt.scatter(i, rev_accuracies[attr], color='red', label='Reverse' if i == 0 else "")

  plt.xticks(range(len(attributes)), attributes, rotation=45)
  plt.ylabel('Accuracy')
  plt.title('Accuracy per Attribute')
  plt.legend()
  plt.tight_layout()
  plt.text(-1, 1.2,"Epochs: " + str(epoch), fontsize=12, color='blue')
  plt.text(1,1.2,"Batch-size: 100", fontsize=12, color='red')
  plt.text(3,1.2,"Learning-rate: 0.0028", fontsize=12, color='green')
  plt. text(-1, 1.15, "Using 2D Depthwise Convolutions", fontsize=12, color='black')
  plt. text(-1, 1.10, "Runtime (CPU): " + str(sum1) + " seconds", fontsize=10, color='black')

  plt.show()

def main():
  extract_tar_gz(tar_path, extract_path)
  csv_data = read_csv()
  run_and_plot(csv_data, 'occlusion', epoch=100, lr=0.0032585)
  #run_5_times(csv_data, 'pose')
  #run_all_fwd_rev(csv_data)

if __name__ == "__main__":
    main()