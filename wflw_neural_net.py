# -*- coding: utf-8 -*-
"""WFLW_Neural_Net

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_dXCJkMsFt6vrwoZXCks5hQXr6LpyDq
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers, models
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import cv2
import ast

from google.colab import drive
drive.mount('/content/drive')

"""Extracts annotations from a .tar.gz file into a separate path"""
def extract_tar_gz(tar_path="/content/drive/MyDrive/WFLW_images.tar.gz", extract_path="/content/WFLW_images"):
  import tarfile
  import os

  # Ensure the extraction directory exists
  os.makedirs(extract_path, exist_ok=True)

  with tarfile.open(tar_path, "r:gz") as tar:
      # Extract all contents to the specified path
      tar.extractall(path=extract_path)

  print(f"Extracted contents to: {extract_path}")

#cat = category (folder)
def make_image_list(image_dir="/content/WFLW_images/WFLW_images"):
  import os

  image_cats = os.listdir(image_dir) #list of folders within main dir
  image_files = []

  for cat in image_cats:#loops through folders
      image_folder_path = os.path.join(image_dir, cat) #path to the folder
      if not os.path.isdir(image_folder_path):
          continue
      images = os.listdir(image_folder_path) #list of images within folder
      for image in images:
          image_files.append(image) #list of ALL(6551) image files

""" ###Plots 5 images per folder for 5 image folders###
import cv2
import matplotlib.pyplot as plt

for category in image_cats[0:5]:
    image_folder_path = os.path.join(image_dir, category)
    if not os.path.isdir(image_folder_path):
        continue
    images_in_cat = os.listdir(image_folder_path)
    for image_file in images_in_cat[0:5]: # Limit to first 5 images per category for demonstration
        img_path = os.path.join(image_folder_path, image_file)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        plt.imshow(img)
        plt.title(category)
        plt.axis("off")
        plt.show()
"""

"""TXT > Dictionary > CSV"""
def read_as_csv(input_file, output_file):
  import csv

  data = open(input_file)
  see_data = data.read() #read full txt file

  list_of_str = see_data.split('\n') #split by line
  list_of_dicts = []

  for line in list_of_str:
      if line.strip(): #skip empty lines
          res = ast.literal_eval(line)
          list_of_dicts.append(res) #makes a list of dictionaries

  with open(output_file, 'w', newline='') as csvfile:
      writer = csv.DictWriter(csvfile, fieldnames=list_of_dicts[0].keys())
      writer.writeheader()
      writer.writerows(list_of_dicts) #writing rows from the list of dictionaries to the csv

  csv_data = pd.read_csv('saving.csv')
  print("Displaying the few lines of csv file: ")
  display(csv_data.head())
  return csv_data



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Flatten,Dropout

model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255, input_shape=(96, 96, 3)),

    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),

    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification
])

def load_images(csv_data, image_dir="/content/WFLW_images/WFLW_images"):
  loaded_images = []
  for i in range(len(csv_data)):
    img_path = csv_data['file_name'][i] #creates a path using the folder(drive) and the filename as found in the csv
    img_path = image_dir + "/" + img_path
    if os.path.exists(img_path): # Check if the file exists
      img = cv2.imread(img_path)
      img = cv2.resize(img, (96, 96))
      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      if img is not None:
        loaded_images.append(img)
      else:
        print(f"Image not found: {img_path}")
        loaded_images.append(None)

  return loaded_images

def make_NN(attribute):
    tf.keras.optimizers.Adam(learning_rate=0.0001)

    valid_indices = [i for i, img in enumerate(loaded_images) if img is not None]
    filtered_csv_data = csv_data.iloc[valid_indices].reset_index(drop=True)


    training_labels = filtered_csv_data[attribute].to_numpy()
    # Convert labels to a numerical format (e.g., integers)
    training_labels_numerical = training_labels.astype(int)


    # Convert the list of images to a NumPy array
    training_images_array = np.array(loaded_images)


    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC()]
    )

    history=model.fit(
        training_images_array,
        training_labels_numerical,
        batch_size=50,
        epochs=15
    )

def main():
  extract_tar_gz(tar_path, extract_path)
  make_image_list(image_dir)
  csv_data = read_as_csv('saving.txt', 'saving.csv')
  load_images(csv_data, image_dir)
  make_NN('make_up')

if __name__ == "__main__":
    main()
